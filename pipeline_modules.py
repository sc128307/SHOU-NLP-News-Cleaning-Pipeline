import os
import re
import json
import pandas as pd
import nltk
import torch
import transformers
import unicodedata
import numpy as np
import platform
import psutil
import gc
from striprtf.striprtf import rtf_to_text
from transformers import AutoTokenizer, AutoModelForTokenClassification
from sentence_transformers import SentenceTransformer, util

transformers.logging.set_verbosity_error()

# --- ä¾èµ–æ£€æŸ¥ ---
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt", quiet=True)


# ==================================================
# å·¥å…·ç±»: è®¾å¤‡ç®¡ç†
# ==================================================
class DeviceManager:
    @staticmethod
    def get_optimal_device():
        """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è¿è¡Œè®¾å¤‡: cuda, mps, æˆ– cpu"""
        device = "cpu"
        info = {"type": "cpu", "vram": 0, "desc": "Standard Processing Unit"}

        # A. æ£€æµ‹ NVIDIA GPU
        if torch.cuda.is_available():
            device = "cuda"
            try:
                device_name = torch.cuda.get_device_name(0)
                vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                info = {
                    "type": "cuda",
                    "vram": round(vram_gb, 2),
                    "desc": device_name,
                }
            except:
                info["desc"] = "NVIDIA GPU (Unknown)"

        # B. æ£€æµ‹ Apple Silicon (M1/M2/M3)
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device = "mps"
            # è·å–ç³»ç»Ÿå†…å­˜ä½œä¸ºå‚è€ƒ
            total_mem = psutil.virtual_memory().total / 1024**3
            info = {
                "type": "mps",
                "vram": round(total_mem, 1),
                "desc": "Apple Silicon (Metal)",
            }

        return device, info

    @staticmethod
    def get_model_kwargs(device):
        """æ ¹æ®è®¾å¤‡è¿”å›æ¨¡å‹åŠ è½½å‚æ•°"""
        kwargs = {}
        if device == "cuda":
            # æ˜¾å­˜å¤Ÿçš„è¯å¯ä»¥ç”¨ float16 åŠ é€Ÿ
            kwargs = {"torch_dtype": torch.float16}
        elif device == "mps":
            # Mac ç›®å‰å»ºè®®ä½¿ç”¨ float32 ä¿è¯å…¼å®¹æ€§ï¼Œæˆ–è€…å°è¯• float16
            kwargs = {"torch_dtype": torch.float32}
        else:
            # CPU å¿…é¡» float32
            kwargs = {"torch_dtype": torch.float32}
        return kwargs


# ==================================================
# å·¥å…·ç±»: æ–‡æœ¬æ ¼å¼åŒ–
# ==================================================
class TextFormatter:
    @staticmethod
    def format_text(text):
        if not text:
            return ""
        text = unicodedata.normalize("NFKC", text)
        text = text.replace("\xa0", " ").replace("\u3000", " ").replace("\u200b", "")
        lines = [line.strip() for line in text.splitlines()]
        text = "\n".join(lines)
        text = re.sub(r"(\w+)\s+([.,;:?!])", r"\1\2", text)
        text = re.sub(r" +", " ", text)
        text = re.sub(r"\n{3,}", "\n\n", text)
        print(f"âœ… TextFormatter ran on {len(text)} chars")  # Debug ä¿¡å·
        return text.strip()


# ==================================================
# æ¨¡å— 1: RTF å¤„ç†ä¸åŸºç¡€æ¸…æ´—
# ==================================================
class RTFHandler:
    @staticmethod
    def to_text(file_path):
        try:
            with open(file_path, "rb") as f:
                content = f.read().decode("cp1252", errors="ignore")

            # 1. æ ¸å¿ƒä¿®å¤ï¼šSPh Media ç‰¹æœ‰æ ¼å¼
            content = content.replace(r"\u169?", "(c)")
            content = content.replace(r"{\b", r" {\b").replace(r"{\field", r" {\field")

            # 2. ç»“æ„ç‰©ç†æ–­è¡Œ
            content = content.replace("}{", "} \n {")
            content = re.sub(r"(?<!\\)\\par(?![a-zA-Z])", r"\\par\n", content)
            content = content.replace(r"\par}", r"\par\n}")

            # 3. è½¬æ¢ä¸ºçº¯æ–‡æœ¬
            text = rtf_to_text(content, errors="ignore")

            # 4. åŸºç¡€æ¸…æ´—
            text = text.replace("\r\n", "\n").replace("\r", "\n")

            # ä¿®å¤ CamelCase ç²˜è¿ (e.g. "TheGovernment" -> "The Government")
            text = re.sub(r"([a-z])([A-Z])", r"\1 \2", text)

            text = re.sub(r"\[([a-zA-Z])\]", r"\1", text)
            text = text.replace("â€™", "'").replace("â€˜", "'")
            text = re.sub(r'(READ:.*?)(")', r"\1\n\2", text)

            # æ³›åŒ–ç‰ˆ Header/Body ç²˜è¿åˆ‡å‰²
            patterns = [
                r"(Limited)\s+([A-Z])",
                r"(Corporation)\s+([A-Z])",
                r"(Corp\.?)\s+([A-Z])",
                r"(Inc\.?)\s+([A-Z])",
                r"(Agency)\s+([A-Z])",
                r"(Reserved\.?)\s+([A-Z])",
                r"(Commission)\s+([A-Z])",
                r"(Bhd\.?)\s+([A-Z])",
                r"(English)\s+(Â©|Copyright|\(c\))",
            ]
            for pat in patterns:
                text = re.sub(pat, r"\1\n\n\2", text)

            # 5. æœ€ç»ˆæ•´ç†
            text = re.sub(r"\n{3,}", "\n\n", text)
            text = re.sub(r"[ \t]{2,}", " ", text)

            return text.strip()
        except Exception as e:
            print(f"âŒ RTF Error {file_path}: {e}")
            return ""


# ==================================================
# ç»“æ„æ€§å™ªéŸ³æ¸…æ´— (æ•´ç¯‡åˆ é™¤)
# ==================================================
class StructuralCleaner:
    def __init__(self):
        # è·³è¿‡å¤´æ¡ç®€æŠ¥ç±»å†…å®¹ (Briefing / Update)
        self.SKIP_BRIEFING_PATTERN = re.compile(
            r"^\s*(Morning Briefing|Evening Update|Today's headlines|News in 5 minutes)",
            re.IGNORECASE,
        )

    def is_skippable(self, text):
        # æ£€æŸ¥å‰ 5 è¡Œæ˜¯å¦å‘½ä¸­è·³è¿‡è§„åˆ™
        header_sample = " ".join(text.splitlines()[:5])
        return bool(self.SKIP_BRIEFING_PATTERN.search(header_sample))


# ==================================================
# æ¨¡å— 2: ç»“æ„ä¸å…ƒæ•°æ®æå– (Title/Date/Source)
# ==================================================
class MetaExtractor:
    def __init__(self):
        # ä¸¥æ ¼æ—¥æœŸæ­£åˆ™
        self.date_pattern = re.compile(
            r"(\d{1,2}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4})",
            re.IGNORECASE,
        )

    def analyze_structure(self, text):
        """
        è¿”å›: (header_end_char, footer_start_char, metadata_dict)
        1. Header: ä¾ç„¶ä½¿ç”¨ç‰¹å¾æŸ¥æ‰¾ (Date/Source)
        2. Footer: ç›´æ¥å®šä½åˆ°å€’æ•°ç¬¬ 2 ä¸ªéç©ºè¡Œ (Blind Cut)
        """
        lines_info = []
        cursor = 0
        for line in text.splitlines(keepends=True):
            stripped = line.strip()
            lines_info.append(
                {
                    "text": line,
                    "stripped": stripped,
                    "start": cursor,
                    "end": cursor + len(line),
                }
            )
            cursor += len(line)

        if not lines_info:
            return 0, len(text), {"title": "", "date": "", "source": ""}

        # --- A. Title ---
        title = lines_info[0]["text"].strip()

        # --- B. Header Analysis ---
        header_end_line_idx = 0
        date = ""
        source = ""

        # 1. æ‰¾ Date (Header çš„æ ¸å¿ƒé”šç‚¹)
        date_idx = -1
        # æ‰«æå‰ 20 è¡Œ (V2 é€»è¾‘)
        scan_limit = min(len(lines_info), 20)

        for i in range(scan_limit):
            line_text = lines_info[i]["text"]
            if self.date_pattern.search(line_text):
                date = self.date_pattern.search(line_text).group(0)
                date_idx = i
                break

        # 2. æ‰¾ Source (åœ¨ Date ä¹‹å)
        source_idx = -1
        if date_idx != -1:
            for k in range(date_idx + 1, scan_limit):
                cand = lines_info[k]["text"].strip()
                cand_lower = cand.lower()

                # æ’é™¤åˆ—è¡¨
                if any(
                    x in cand_lower
                    for x in ["copyright", "(c)", "Â©", "tagalog", "words", "english"]
                ):
                    continue
                if re.match(r"^[a-z0-9]+$", cand_lower):  # æ’é™¤çº¯ä»£ç 
                    continue
                if len(cand) > 50:  # Source é€šå¸¸ä¸é•¿
                    break

                source = cand
                source_idx = k
                break

        # 3. æ‰¾ Copyright
        copyright_idx = -1
        for k in range(date_idx + 1 if date_idx != -1 else 0, scan_limit):
            cand_lower = lines_info[k]["text"].lower()
            if any(
                x in cand_lower
                for x in ["copyright", "(c)", "Â©", "all rights reserved"]
            ):
                copyright_idx = k
                break

        # 4. å†³ç­– Header ç»“æŸä½ç½®
        # ä¼˜å…ˆçº§ï¼šCopyright > Source > Date
        if copyright_idx != -1:
            header_end_line_idx = copyright_idx + 1
        elif source_idx != -1:
            header_end_line_idx = source_idx + 1
        elif date_idx != -1:
            header_end_line_idx = date_idx + 1
        else:
            header_end_line_idx = 1  # åªæœ‰æ ‡é¢˜

        # è®¡ç®— Header å­—ç¬¦ä½ç½®
        if header_end_line_idx > 0:
            safe_idx = min(header_end_line_idx - 1, len(lines_info) - 1)
            target_line = lines_info[safe_idx]
            header_end_char = target_line["start"] + len(
                target_line["text"].rstrip("\r\n")
            )
        else:
            header_end_char = lines_info[0]["end"]

        # --- C. Footer Start Detection ---
        non_empty_lines_indices = [
            i for i, info in enumerate(lines_info) if info["stripped"]
        ]

        # å¦‚æœå…¨æ–‡å°‘äº 4 è¡Œï¼Œå¯èƒ½å°±ä¸å­˜åœ¨ Footer æˆ–è€…å…¨æ–‡éƒ½æ˜¯ Footerï¼Œä¿å®ˆèµ·è§è®¾ä¸ºæœ«å°¾
        if len(non_empty_lines_indices) <= 3:
            footer_start_char = len(text)
        else:
            # æ‰¾åˆ°å€’æ•°ç¬¬äºŒè¡Œéç©ºè¡Œçš„ç´¢å¼•
            # [-1] æ˜¯æœ€åä¸€è¡Œ (Document ID)
            # [-2] æ˜¯å€’æ•°ç¬¬äºŒè¡Œ (Source) -> è¿™é‡Œæ˜¯ Footer å¼€å§‹çš„åœ°æ–¹
            footer_start_idx = non_empty_lines_indices[-2]
            footer_start_char = lines_info[footer_start_idx]["start"]

        if header_end_line_idx > 0:
            # ä¸ºäº†é˜²æ­¢ Header å’Œ Footer é‡å  (æ–‡ç« æçŸ­çš„æƒ…å†µ)
            # æˆ‘ä»¬å– header_end å’Œ footer_start çš„è¾ƒå°å€¼
            safe_idx = min(header_end_line_idx - 1, len(lines_info) - 1)
            calculated_header_end = lines_info[safe_idx]["start"] + len(
                lines_info[safe_idx]["text"].rstrip("\r\n")
            )
            header_end_char = min(calculated_header_end, footer_start_char)
        else:
            header_end_char = lines_info[0]["end"]

        return (
            header_end_char,
            footer_start_char,
            {"title": title, "date": date, "source": source},
        )


# ==================================================
# æ¨¡å— 3: AI æ¸…æ´— (æ··åˆæ¶æ„ï¼šAI + Regex + å¥å­å¹³æ»‘)
# ==================================================
class NERCleaner:
    def __init__(self, model_configs):
        # 1. è®¾å¤‡é€‰æ‹©
        self.device, self.device_info = DeviceManager.get_optimal_device()
        print(f"ğŸ¤– Noise Cleaner (DeBERTa) running on: {self.device_info['desc']}")

        self.tokenizer = None
        self.model = None

        # ç»“æ„æ€§å™ªéŸ³æ­£åˆ™
        self.PAT_STRUCTURAL_NOISE = [
            re.compile(
                r"(?:(?<=[.!?])\s*)?(More\s+On\s+This\s+Topic|Related\s+Stor(?:y|ies)).*?$",
                re.IGNORECASE | re.MULTILINE,
            ),
            re.compile(
                r"(?:(?<=[.!?])\s*)?(READ\s+MORE\s+(?:HERE|ABOUT)|Click\s+here\s+to\s+read).*?(?=\n\n|$)",
                re.IGNORECASE | re.DOTALL,
            ),
            re.compile(
                r"Sign\s+up\s+for\s+the\s+ST\s+Asian\s+Insider\s+newsletter.*?(?=\n\n|$)",
                re.IGNORECASE,
            ),
            re.compile(
                r"Disclaimer:\s+The\s+Above\s+Content\s+is\s+Auto-Translated.*",
                re.IGNORECASE | re.DOTALL,
            ),
            re.compile(r"\[Category:.*?\]", re.IGNORECASE),
            # 1. å°å°¼/è¯„è®ºæ–‡ç« ç»“å°¾çš„ Bio åˆ†å‰²çº¿
            # é‡åˆ° "______" æˆ– "-----" å°±æŠŠåé¢å…¨åˆ äº†
            re.compile(r"(?m)^\s*[_\-]{5,}\s*[\s\S]*$"),
            # 2. å¸¸è§çš„å…è´£å£°æ˜ (ä½œä¸ºè¡¥å……ï¼Œé˜²æ­¢åˆ†å‰²çº¿æ¼æ‰)
            re.compile(
                r"(?i)^The\s+views\s+expressed\s+are\s+(personal|solely\s+those\s+of\s+the\s+author).*$",
                re.MULTILINE,
            ),
            # æ–°å¢è§„åˆ™å¯ä»¥ç»§ç»­æ·»åŠ ...
        ]

        # 2. åŠ è½½æ¨¡å‹
        model_path = model_configs.get("NOISE_CAPTION")

        if not model_path:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„
            model_path = "microsoft/mdeberta-v3-base"
            print(
                f"âš ï¸ Warning: 'NOISE_CAPTION' not in config, using default: {model_path}"
            )

        print(f"   â†³ Loading DeBERTa from {model_path} ...")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)

            # æ ¹æ®è®¾å¤‡é€‰æ‹©åŠ è½½å‚æ•°
            model_kwargs = DeviceManager.get_model_kwargs(self.device)

            self.model = AutoModelForTokenClassification.from_pretrained(
                model_path, **model_kwargs
            )
            self.model.to(self.device)
            self.model.eval()
            self.noise_label_id = 1
        except Exception as e:
            print(f"âŒ DeBERTa Model Load Failed: {e}")

    def clean(self, raw_text, header_end, footer_start, protected_keywords=None):
        """
        raw_text: å…¨æ–‡
        header_end / footer_start: æ­£æ–‡çš„èµ·æ­¢ä½ç½®
        protected_keywords: å¦‚æœå¥å­åŒ…å«è¿™äº›è¯ï¼Œå¼ºåˆ¶ä¸è¿›è¡ŒAIæ•´å¥åˆ é™¤
        """
        # 1. æå–æ­£æ–‡ä¸»ä½“
        if header_end >= footer_start:
            return "", []

        raw_body = raw_text[header_end:footer_start].lstrip()
        # è®¡ç®—åç§»é‡ä»¥ä¾¿æœ€åè¿”å› span (è™½ç„¶ç°åœ¨ä¸»è¦ç”¨ text)
        skipped_len = len(raw_text[header_end:footer_start]) - len(raw_body)
        body_offset = header_end + skipped_len

        all_deleted_spans = []  # ç”¨äºæ”¶é›†æ‰€æœ‰è¢«åˆ é™¤çš„ç‰‡æ®µ

        # =========================================
        # 1. æ‰§è¡Œ AI æ‰«æ (åªè®°å½•ä½ç½®ï¼Œä¸ç”Ÿæˆæ–‡æœ¬)
        # =========================================
        if self.model:
            paragraphs = raw_body.splitlines(keepends=True)
            current_rel_pos = 0

            for para in paragraphs:
                if len(para.strip()) < 5:
                    current_rel_pos += len(para)
                    continue

                # è®¡ç®—ç»å¯¹åæ ‡
                abs_offset = body_offset + current_rel_pos

                # è·å– AI è®¤ä¸ºè¯¥åˆ çš„ç‰‡æ®µ
                _, deleted_in_para = self._ai_clean_paragraph(
                    para, abs_offset, protected_keywords
                )
                all_deleted_spans.extend(deleted_in_para)
                current_rel_pos += len(para)

        # =========================================
        # 2. æ‰§è¡Œ Regex æ‰«æ
        # =========================================
        # æˆ‘ä»¬åœ¨ raw_body ä¸Šç›´æ¥è·‘æ­£åˆ™ï¼Œæ‰¾åˆ°æ‰€æœ‰ç»“æ„æ€§å™ªéŸ³
        for pat in self.PAT_STRUCTURAL_NOISE:
            for match in pat.finditer(raw_body):
                start_idx = match.start()
                end_idx = match.end()

                # è®°å½• Regex åˆ é™¤çš„ç‰‡æ®µ
                all_deleted_spans.append(
                    {
                        "start": body_offset + start_idx,
                        "end": body_offset + end_idx,
                        "type": "STRUCTURAL_NOISE (Regex)",  # å‰ç«¯æ˜¾ç¤ºä¸ºç»“æ„æ€§å™ªéŸ³
                        "score": 1.0,
                        "text": match.group(),
                    }
                )

        # =========================================
        # 3. æ–‡æœ¬é‡ç»„
        # =========================================
        # æœ‰äº†æ‰€æœ‰çš„â€œåƒåœ¾åæ ‡â€ (AI + Regex)ï¼Œç°åœ¨æŠŠå®ƒä»¬åˆå¹¶ï¼Œ
        # ç„¶åä» raw_body ä¸­æŒ–æ‰è¿™äº›éƒ¨åˆ†ï¼Œç”Ÿæˆæœ€ç»ˆæ–‡æœ¬ã€‚

        # A. å°†ç»å¯¹åæ ‡è½¬å›ç›¸å¯¹åæ ‡ (Relative to raw_body)
        spans_relative = []
        for span in all_deleted_spans:
            rel_start = span["start"] - body_offset
            rel_end = span["end"] - body_offset
            # ç¡®ä¿åæ ‡åœ¨ body èŒƒå›´å†…
            rel_start = max(0, min(rel_start, len(raw_body)))
            rel_end = max(0, min(rel_end, len(raw_body)))
            if rel_start < rel_end:
                spans_relative.append((rel_start, rel_end))

        # B. åˆå¹¶é‡å åŒºé—´ (é˜²æ­¢ AI å’Œ Regex åˆ äº†åŒä¸€æ®µå¯¼è‡´åˆ‡ç‰‡é”™è¯¯)
        spans_relative.sort(key=lambda x: x[0])
        merged_spans = []
        if spans_relative:
            curr_start, curr_end = spans_relative[0]
            for next_start, next_end in spans_relative[1:]:
                if next_start < curr_end:  # é‡å æˆ–ç›¸æ¥
                    curr_end = max(curr_end, next_end)
                else:
                    merged_spans.append((curr_start, curr_end))
                    curr_start, curr_end = next_start, next_end
            merged_spans.append((curr_start, curr_end))

        # C. æ‰§è¡Œè£å‰ª (Slicing)
        final_parts = []
        last_pos = 0
        for start, end in merged_spans:
            # ä¿ç•™ä¸Šä¸€æ®µç»“æŸåˆ°è¿™ä¸€æ®µå¼€å§‹ä¹‹é—´çš„å†…å®¹ (å³æ­£æ–‡)
            final_parts.append(raw_body[last_pos:start])
            last_pos = end
        # åŠ ä¸Šæœ€åå‰©ä½™çš„éƒ¨åˆ†
        final_parts.append(raw_body[last_pos:])

        final_body = "".join(final_parts)
        final_body = re.sub(r"\n{3,}", "\n\n", final_body).strip()

        # è¿”å›é‡ç»„åçš„æ–‡æœ¬ + å®Œæ•´çš„é«˜äº®åˆ—è¡¨
        return final_body, all_deleted_spans

    def _ai_clean_paragraph(self, text, offset, protected_keywords=None):
        if not text.strip():
            return text, []

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            return_offsets_mapping=True,
            padding="longest",
        ).to(self.device)
        offsets = inputs["offset_mapping"][0].cpu().numpy()

        with torch.no_grad():
            outputs = self.model(
                input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"]
            )
            predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()

        n = len(text)
        char_is_noise = np.zeros(n, dtype=bool)
        for i, (start, end) in enumerate(offsets):
            if start == end:
                continue
            if predictions[i] == self.noise_label_id:
                char_is_noise[start:end] = True

        return self._apply_sentence_logic(
            text, char_is_noise, offset, protected_keywords
        )

    def _apply_sentence_logic(self, text, char_mask, offset, protected_keywords):
        import re

        sentences_spans = []
        start = 0
        # ä¿æŒä¿®å¤åçš„åˆ†å¥é€»è¾‘
        for match in re.finditer(r"(?<=[.!?])\s+", text):
            end = match.end()
            sentences_spans.append((start, end))
            start = match.end()
        sentences_spans.append((start, len(text)))

        keywords_lower = (
            [k.lower() for k in protected_keywords] if protected_keywords else []
        )
        final_chunks = []  # è¿™ä¸ªå˜é‡å…¶å®åœ¨ V4.0 é‡Œåªèµ·è¾…åŠ©ä½œç”¨äº†ï¼Œä½†ä¿ç•™ä»¥å…¼å®¹æ¥å£
        deleted_spans = []

        for sent_start, sent_end in sentences_spans:
            sent_text = text[sent_start:sent_end]
            if not sent_text.strip():
                final_chunks.append(sent_text)
                continue

            sent_len = sent_end - sent_start
            sent_mask = char_mask[sent_start:sent_end]
            noise_ratio = np.sum(sent_mask) / sent_len if sent_len > 0 else 0

            def record_deletion(reason):
                deleted_spans.append(
                    {
                        "start": offset + sent_start,
                        "end": offset + sent_end,
                        "type": f"AI_NOISE ({reason})",
                        "score": 0.99,
                        "text": sent_text,
                    }
                )

            is_kept = True

            if any(kw in sent_text.lower() for kw in keywords_lower):
                pass  # Keep
            elif noise_ratio > 0.4:
                record_deletion("Ratio > 0.4")
                is_kept = False
            elif (
                "PHOTO:" in sent_text or "Source:" in sent_text
            ) and noise_ratio > 0.1:
                record_deletion("Visual/Source Trigger")
                is_kept = False

            if is_kept:
                final_chunks.append(sent_text)

        return "".join(final_chunks), deleted_spans

    def release_memory(self):
        print("ğŸ§¹ Releasing NER model memory...")
        if hasattr(self, "model"):
            del self.model
        if hasattr(self, "tokenizer"):
            del self.tokenizer
        gc.collect()
        if self.device == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        elif self.device == "mps":
            torch.mps.empty_cache()
        print("âœ… NER memory released.")


# ==================================================
# æ¨¡å— 4a: ç›¸å…³æ€§è¿‡æ»¤å™¨ (è¯­ä¹‰æ¸…æ´—)
# ==================================================
class RelevanceFilter:
    def __init__(self):
        self.df = None
        # 1. ç»å¯¹ç™½åå• (æƒé‡æœ€é«˜ - ä¸€ç¥¨é€šè¿‡)
        self.WHITELIST_PHRASES = [
            r"Communist Party of China",
            r"Chinese Communist Party",
            r"General Secretary of the CPC",
            r"General Secretary of the CCP",
            r"ruling party of China",
            r"CCP regime",
            r"Beijing's ruling CPC",
        ]

        # 2. åŸºç¡€é”šç‚¹ (å¿…é¡»åŒ…å«å…¶ä¸­ä¹‹ä¸€æ‰èƒ½è¿›å…¥ä¸‹ä¸€è½®)
        self.CHINA_ANCHORS = [
            "china",
            "chinese",
            "beijing",
            "xi jinping",
            "prc",
            "ccp",
            "cpc",
            "south china sea",
            "asean",
        ]

        # 3. ç°ä»£åŒ–ä¸“ç”¨æ­£åˆ™ (é’ˆå¯¹ Topic: Modernization)
        self.MODERNIZATION_PATTERNS = [
            re.compile(r"Chinese(-|\s+)style\s+moderni[sz]ation", re.IGNORECASE),
            re.compile(r"Chinese\s+path\s+to\s+moderni[sz]ation", re.IGNORECASE),
            re.compile(r"Chinese\s+moderni[sz]ation", re.IGNORECASE),
        ]

        # 4. å±€éƒ¨æ¶ˆæ­§ä¹‰æ­£åˆ™ (é’ˆå¯¹ CCP / CPC å¤šä¹‰è¯)
        self.LOCAL_NOISE_PATTERNS = [
            # è²å¾‹å®¾ CCP (Cultural Center)
            re.compile(r"Cultural\s+Center\s+of\s+the\s+Philippines", re.IGNORECASE),
            re.compile(
                r"\bCCP\s+(Complex|Main Theater|Little Theater|Studio|Dance|Ballet|Orchestra|Visual Arts|Children's Biennale)\b",
                re.IGNORECASE,
            ),
            re.compile(
                r"\b(at|visit|ticket|show|exhibit|perform)\s+(at\s+)?(the\s+)?CCP\b",
                re.IGNORECASE,
            ),
            # è²å¾‹å®¾/å…¶ä»– CPC (Child Protection / Community)
            re.compile(r"Child\s+Protection\s+Center", re.IGNORECASE),
            re.compile(r"Valenzuela\s+City\s+CPC", re.IGNORECASE),
            re.compile(
                r"\bCPC\s+(comprising|staffed|team|doctors|social workers|barangay)\b",
                re.IGNORECASE,
            ),
            # é©¬æ–° CPC (åˆ‘æ³•)
            re.compile(r"\bSection\s+\d+\s+of\s+(the\s+)?CPC\b", re.IGNORECASE),
            re.compile(
                r"\b(charged|investigated|detained|court)\s+under\s+(the\s+)?CPC\b",
                re.IGNORECASE,
            ),
            re.compile(r"\bCPC\s+(Code|Act|Section|provision)\b", re.IGNORECASE),
        ]

    # å¿«é€Ÿç­›é€‰
    def is_relevant(self, text, title="", topic_mode="GENERAL"):
        combined_text = title + "\n" + text
        combined_lower = combined_text.lower()

        # 1. ç»å¯¹ç™½åå•
        for phrase in self.WHITELIST_PHRASES:
            if phrase.lower() in combined_lower:
                return True, "WHITELIST_MATCH"

        # 2. å±€éƒ¨æ¶ˆæ­§ (Regex)
        for pat in self.LOCAL_NOISE_PATTERNS:
            if pat.search(combined_text):
                return False, f"NOISE_PATTERN: {pat.pattern}"

        # 3. è¯é¢˜åˆ†æµ
        if topic_mode == "MODERNIZATION":
            for pat in self.MODERNIZATION_PATTERNS:
                if pat.search(combined_text):
                    return True, "MODERNIZATION_MATCH"
            pass  # ç»§ç»­èµ°åç»­çš„ China æ£€æŸ¥

        elif topic_mode == "STRICT_CPC":
            # å¿…é¡»æœ‰ç¼©å†™
            has_abbr = (
                "ccp" in combined_lower.split() or "cpc" in combined_lower.split()
            )
            if not has_abbr:
                return False, "NO_CPC_ABBR"
            # åªè¦æœ‰ç¼©å†™ï¼Œå°±æ”¾è¡Œç»™è¯­ä¹‰æ¨¡å‹å»åˆ¤æ–­æ˜¯ä¸æ˜¯"Cultural Center"
            return True, "CPC_ABBR_FOUND"

        # 4. é€šç”¨é—¨æ§›ï¼šæ£€æŸ¥åŸºç¡€é”šç‚¹
        # åªè¦åŒ…å« "China", "Beijing" ç­‰è¯ï¼Œå°±æ”¾è¡Œè¿›å…¥è¯­ä¹‰åˆ†æ
        for anchor in self.CHINA_ANCHORS:
            if anchor in combined_lower:
                return True, "ANCHOR_MATCH"

        return False, "NO_CHINA_KEYWORDS"


## ==================================================
# æ¨¡å— 4b: è¯­ä¹‰ç›¸å…³æ€§è¿‡æ»¤å™¨ (Sentence-BERT)
# ==================================================
class SemanticRelevanceFilter:
    def __init__(self, config, threshold=0.15):
        """
        threshold: æ­£å‘ç›¸ä¼¼åº¦çš„æœ€ä½é—¨æ§›ã€‚å³ä½¿æ²¡è§¦çŠ¯è´Ÿå‘è§„åˆ™ï¼Œå¦‚æœç¦»æ”¿æ²»å¤ªè¿œä¹Ÿä¸è¦ã€‚
        """
        self.config = config
        self.threshold = threshold

        # 1. è®¾å¤‡é€‰æ‹©
        self.device, self.device_info = DeviceManager.get_optimal_device()
        print(f"ğŸ§  Semantic Engine (MiniLM) running on: {self.device_info['desc']}")

        # 2. è·å–æ¨¡å‹è·¯å¾„
        model_path = config.get(
            "SEMANTIC_MODEL", "sentence-transformers/all-MiniLM-L6-v2"
        )

        # 3. åŠ è½½æ¨¡å‹
        opt_kwargs = DeviceManager.get_model_kwargs(self.device)
        print(f"   â†³ Loading Semantic Model from: {model_path} ...")

        try:
            # å°è¯•æœ¬åœ°åŠ è½½
            self.model = SentenceTransformer(
                model_path, device=self.device, model_kwargs=opt_kwargs
            )
        except Exception as e:
            print(f"âŒ Failed to load Semantic Model: {e}")
            # å°è¯•ä» HuggingFace ä¸‹è½½
            print("   â†³ Fallback: Downloading 'all-MiniLM-L6-v2' from HuggingFace...")
            self.model = SentenceTransformer("all-MiniLM-L6-v2", device=self.device)

        # 4. åŠ è½½è§„åˆ™ (é…ç½®)
        self.config_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "semantic_config.json"
        )
        self.load_concepts()
        self.update_embeddings()

    # === å®šä¹‰é”šç‚¹ ===
    def load_concepts(self):
        """ä» JSON åŠ è½½æ¦‚å¿µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼"""
        defaults = {
            # 1. æˆ‘ä»¬æƒ³è¦çš„å†…å®¹ (æ”¿æ²»ã€å¤–äº¤ã€æˆ˜ç•¥ç»æµ)
            "positive": [
                "Diplomacy and bilateral relations between countries",
                "Government official visits and high-level meetings",
                "Belt and Road Initiative and infrastructure projects",
                "South China Sea disputes and maritime security",
                "International trade agreements and economic cooperation",
                "Chinese state-owned enterprises investment",
                "Foreign ministry statements and embassies",
                "Political ideology and party congress",
            ],
            "negative": [
                # 2. æˆ‘ä»¬ä¸æƒ³è¦çš„å†…å®¹ (çº¯å•†ä¸šã€ç”Ÿæ´»ã€å¹¿å‘Š)
                "Commercial banking awards and financial performance reports",
                "Retail promotions, shopping, and restaurant food reviews",
                "Travel holiday packages and tourism advertisements",
                "Sports match results and athlete news",
                "Entertainment, celebrity gossip, and movies",
                "Routine crime reports and local accidents",
                "Stock market fluctuations and corporate shareholders meeting",
                "Art exhibitions and cultural performances tickets",
                "Newspaper publisher notes, editorial disclaimers, and advertising supplements",
            ],
        }

        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.positive_concepts = data.get("positive", defaults["positive"])
                    self.negative_concepts = data.get("negative", defaults["negative"])
                print("âœ… Loaded semantic config from file.")
            except Exception as e:
                print(f"âš ï¸ Config load failed ({e}), using defaults.")
                self.positive_concepts = defaults["positive"]
                self.negative_concepts = defaults["negative"]
        else:
            print("â„¹ï¸ No config file found, creating default.")
            self.positive_concepts = defaults["positive"]
            self.negative_concepts = defaults["negative"]
            self.save_concepts()

    # ä¿å­˜å½“å‰æ¦‚å¿µåˆ° JSON
    def save_concepts(self):
        try:
            data = {
                "positive": self.positive_concepts,
                "negative": self.negative_concepts,
            }
            with open(self.config_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ Failed to save config: {e}")

    # å½“æ¦‚å¿µæ”¹å˜æ—¶ï¼Œé‡æ–°è®¡ç®— Embeddings
    def update_embeddings(self):
        if hasattr(self, "model"):
            self.pos_embedding = self.model.encode(
                " ".join(self.positive_concepts), convert_to_tensor=True
            )
            self.neg_embedding = self.model.encode(
                " ".join(self.negative_concepts), convert_to_tensor=True
            )

        # é¢„è®¡ç®—é”šç‚¹çš„å‘é‡ (åŠ é€Ÿåç»­æ¨ç†)
        # æˆ‘ä»¬æŠŠæ‰€æœ‰æ­£å‘æ¦‚å¿µæ‹¼æˆä¸€ä¸ªå¤§çš„è¯­ä¹‰å‘é‡ï¼Œè´Ÿå‘åŒç†
        self.pos_embedding = self.model.encode(
            " ".join(self.positive_concepts), convert_to_tensor=True
        )
        self.neg_embedding = self.model.encode(
            " ".join(self.negative_concepts), convert_to_tensor=True
        )

    def is_relevant(self, text, title=""):
        """
        è¿”å›: (bool, reason, scores)
        """
        # ç»„åˆæ ‡é¢˜å’Œæ­£æ–‡çš„å‰ 800 ä¸ªå­—ç¬¦ (å¼€å¤´é€šå¸¸åŒ…å«ä¸»æ—¨)
        # æ²¡å¿…è¦è¯»å…¨æ–‡ï¼Œæ—¢çœæ—¶é—´åˆé˜²æ­¢è¢«åæ–‡çš„å™ªéŸ³å¹²æ‰°
        content_snippet = f"{title}. {text[:800]}"

        # è®¡ç®—å½“å‰æ–‡ç« çš„å‘é‡
        doc_embedding = self.model.encode(content_snippet, convert_to_tensor=True)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        score_pos = util.cos_sim(doc_embedding, self.pos_embedding).item()
        score_neg = util.cos_sim(doc_embedding, self.neg_embedding).item()

        scores_info = f"[Pos: {score_pos:.3f} | Neg: {score_neg:.3f}]"

        # === åˆ¤å®šé€»è¾‘ ===

        # 1. è´Ÿå‘å‹å€’æ­£å‘ï¼šè™½ç„¶æœ‰ä¸€ç‚¹ç‚¹æ”¿æ²»å‘³ï¼Œä½†æ›´åƒæ˜¯ä¸€ç¯‡å¹¿å‘Š/è´¢æŠ¥
        # æ¯”å¦‚ï¼šBIBD Bank won an award in Beijing. (Beijingæä¾›äº†posåˆ†ï¼Œä½†Bank Awardæä¾›äº†å·¨å¤§çš„negåˆ†)
        if score_neg > score_pos:
            return False, f"SEMANTIC_NOISE {scores_info}"

        # 2. æ­£å‘åˆ†æ•°å¤ªä½ï¼šè¿™æ–‡ç« å¯èƒ½è°éƒ½ä¸æ²¾è¾¹ï¼ˆæ¯”å¦‚è®²æ¾³æ´²å¤©æ°”çš„ï¼‰
        if score_pos < self.threshold:
            return False, f"LOW_RELEVANCE {scores_info}"

        # 3. é€šè¿‡ç­›é€‰
        return True, f"SEMANTIC_MATCH {scores_info}"

    def release_memory(self):
        print("ğŸ§  Releasing Semantic Model (MiniLM) memory...")
        if hasattr(self, "model"):
            del self.model
        if hasattr(self, "pos_embedding"):
            del self.pos_embedding
        if hasattr(self, "neg_embedding"):
            del self.neg_embedding
        gc.collect()
        if self.device == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        elif self.device == "mps":
            try:
                torch.mps.empty_cache()
            except:
                pass
        print("âœ… Semantic Model memory released.")


# ==================================================
# æ¨¡å— 5: æµæ°´çº¿æ§åˆ¶å™¨
# ==================================================
class CorpusPipeline:
    def __init__(self, model_configs):
        # 1. åˆå§‹åŒ–å·¥å…·æ¨¡å—
        self.rtf_handler = RTFHandler()
        self.struct_cleaner = StructuralCleaner()
        self.meta_extractor = MetaExtractor()

        # 2. åˆå§‹åŒ– AI æ¸…æ´—å™¨å’Œè¯­ä¹‰è¿‡æ»¤å™¨
        self.cleaner = NERCleaner(model_configs)
        self.semantic_filter = SemanticRelevanceFilter(model_configs, threshold=0.15)

        # 3. åˆå§‹åŒ–ç›¸å…³æ€§è¿‡æ»¤å™¨
        self.relevance_filter = RelevanceFilter()

    def process_folder(
        self, input_dir, output_base_dir=None, recursive=False, progress_callback=None
    ):
        if self.cleaner is None or self.semantic_filter is None:
            print("âŒ Error: Pipeline models not initialized correctly.")
            return

        all_files = []

        if recursive:
            # === æ¨¡å¼ A: é€’å½’ (Batch Mode) ===
            print(f"ğŸ”„ Scanning RECURSIVELY in: {input_dir}")
            for root, _, files in os.walk(input_dir):
                for f in files:
                    if f.lower().endswith(".rtf"):
                        all_files.append(os.path.join(root, f))
        else:
            # === æ¨¡å¼ B: å•å±‚ (Single Folder Mode) ===
            print(f"âºï¸ Scanning SINGLE LEVEL in: {input_dir}")
            if os.path.exists(input_dir):
                for f in os.listdir(input_dir):
                    full_path = os.path.join(input_dir, f)
                    if os.path.isfile(full_path) and f.lower().endswith(".rtf"):
                        all_files.append(full_path)

        if not all_files:
            print("âš ï¸ No RTF files found.")
            return

        # è¿›åº¦ç»Ÿè®¡
        total_files = len(all_files)
        processed_count = 0
        print(f"ğŸš€ Found {total_files} files.")

        files_by_folder = {}
        for f in all_files:
            folder = os.path.dirname(f)
            if folder not in files_by_folder:
                files_by_folder[folder] = []
            files_by_folder[folder].append(f)

        print(f"ğŸ“‚ Grouped into {len(files_by_folder)} folders.")

        for folder, files in files_by_folder.items():
            # é˜²æ­¢åœ¨æ ¹ç›®å½•ç”Ÿæˆ /output (å¦‚æœæ˜¯é€’å½’æ¨¡å¼)
            if recursive and os.path.normpath(folder) == os.path.normpath(input_dir):
                print(f"â© Skipping root folder output: {folder}")
                continue
            rel_path = os.path.relpath(folder, input_dir)
            out_folder = os.path.join(folder, "output")
            os.makedirs(out_folder, exist_ok=True)

            # å‹å¥½æ˜¾ç¤ºè·¯å¾„
            display_path = rel_path
            if rel_path == ".":
                display_path = f"{os.path.basename(input_dir)} (Root)"

            # æ ¹æ®æ–‡ä»¶å¤¹åç§°åˆ¤æ–­ Topic Mode
            folder_name_lower = os.path.basename(folder).lower()
            topic_mode = "GENERAL_CHINA"  # é»˜è®¤

            if "modern" in folder_name_lower:  # è¦†ç›– modernization, modernisation
                topic_mode = "MODERNIZATION"
            elif (
                "cpc" in folder_name_lower
                or "ccp" in folder_name_lower
                or "party" in folder_name_lower
            ):
                topic_mode = "STRICT_CPC"

            # æ‰“å°ä¸€ä¸‹å½“å‰çš„æ¨¡å¼ï¼Œæ–¹ä¾¿è°ƒè¯•ç¡®è®¤
            print(f"ğŸ“‚ Processing: {display_path} | Mode: {topic_mode}")

            frontend_data_list = []
            csv_logs = []  # è¿›åº¦æ—¥å¿—

            # æ„å»ºä¿æŠ¤è¯åˆ—è¡¨
            protected_kws = []
            try:
                # ä» Gatekeeper è·å–ç™½åå•
                protected_kws.extend(self.relevance_filter.WHITELIST_PHRASES)
                # ä» Semantic Filter è·å–æ­£å‘æ¦‚å¿µé‡Œçš„å…³é”®è¯ (ç®€å•åˆ†è¯)
                # ç®€å•åŠ ä¸€äº›æ ¸å¿ƒè¯ï¼Œä¸ç”¨å¤ªå¤æ‚
                protected_kws.extend(
                    [
                        "modernization",
                        "modernisation",
                        "bilateral",
                        "summit",
                        "relations",
                    ]
                )
                protected_kws = list(set([k for k in protected_kws if len(k) > 2]))
            except Exception as e:
                print(f"âš ï¸ å…³é”®è¯æå–è­¦å‘Š: {e}")

            for rtf_path in files:
                processed_count += 1
                # å‘é€è¿›åº¦ç»™ Electron
                if progress_callback:
                    progress_callback(
                        processed_count,
                        total_files,
                        f"Processing: {os.path.basename(rtf_path)}",
                    )

                # A. è¯»å–
                raw_text = self.rtf_handler.to_text(rtf_path)
                if not raw_text:
                    continue

                temp_title = raw_text.split("\n")[0] if raw_text else ""

                # æ²™æ¼è¿‡æ»¤å™¨
                # === è¿‡æ»¤ç¬¬ä¸€æ­¥ï¼šå…³é”®è¯===
                is_kept_gate, gate_reason = self.relevance_filter.is_relevant(
                    raw_text, temp_title, topic_mode=topic_mode
                )

                if not is_kept_gate:
                    print(
                        f"ğŸš« [Gatekeeper Skipped] {os.path.basename(rtf_path)}: {gate_reason}"
                    )
                    continue

                # === è¿‡æ»¤ç¬¬äºŒæ­¥ï¼šè¯­ä¹‰===
                # åªæœ‰é€šè¿‡äº†ç¬¬ä¸€æ­¥çš„æ–‡ç« æ‰ä¼šè¿›è¿™é‡Œ
                is_kept_sem, sem_reason = self.semantic_filter.is_relevant(
                    raw_text, temp_title
                )

                if not is_kept_sem:
                    print(
                        f"ğŸ—‘ï¸ [Semantic Skipped] {os.path.basename(rtf_path)}: {sem_reason}"
                    )
                    continue

                # print(f"âœ… [Kept] {os.path.basename(rtf_path)}: {sem_reason}")

                # B. è¿‡æ»¤ Briefing
                if self.struct_cleaner.is_skippable(raw_text):
                    continue

                # C. ç»“æ„åˆ†æ
                h_end, f_start, meta = self.meta_extractor.analyze_structure(raw_text)

                # D. NER æ¸…æ´—
                final_clean_body, body_noise = self.cleaner.clean(
                    raw_text, h_end, f_start, protected_keywords=protected_kws
                )

                # æ ¼å¼åŒ– (Formatting)
                final_clean_body = TextFormatter.format_text(final_clean_body)

                # E. æ„å»ºé«˜äº®
                highlights = []
                if h_end > 0:
                    highlights.append({"start": 0, "end": h_end, "type": "HEADER"})
                highlights.extend(body_noise)
                if f_start < len(raw_text):
                    highlights.append(
                        {"start": f_start, "end": len(raw_text), "type": "FOOTER"}
                    )

                # F. ä¿å­˜ TXT
                file_stem = os.path.splitext(os.path.basename(rtf_path))[0]
                if file_stem.startswith("._"):
                    file_stem = file_stem[2:]
                clean_filename = re.sub(r'[\\/*?:"<>|]', "_", file_stem) + ".txt"

                out_txt_path = os.path.join(out_folder, clean_filename)
                content = (
                    f"<title>{meta['title']}</title>\n"
                    f"<date>{meta['date']}</date>\n"
                    f"<source>{meta['source']}</source>\n"
                    f"<body>\n{final_clean_body}\n</body>"
                )
                with open(out_txt_path, "w", encoding="utf-8") as f:
                    f.write(content)

                self._append_to_folder_logs(
                    out_folder,
                    {
                        "filename": clean_filename,
                        "original_text": raw_text,
                        "cleaned_body": final_clean_body,
                        "highlights": [],
                        "metadata": meta,
                    },
                    {
                        "Filename": clean_filename,
                        "Title": meta["title"],
                        "Date": meta["date"],
                        "Source": meta["source"],
                        "Checked": "No",
                    },
                )
                # G. æ•°æ®æ”¶é›†
                frontend_data_list.append(
                    {
                        "filename": clean_filename,
                        "original_text": raw_text,
                        "cleaned_body": final_clean_body,
                        "highlights": highlights,
                        "metadata": meta,
                    }
                )

                # H. æ”¶é›† CSV æ—¥å¿—
                csv_logs.append(
                    {
                        "Filename": clean_filename,
                        "Title": meta["title"],
                        "Date": meta["date"],
                        "Source": meta["source"],
                        "Checked": "No",
                    }
                )

            # ä¿å­˜ JSON
            if frontend_data_list:
                json_path = os.path.join(out_folder, "frontend_diff.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(frontend_data_list, f, ensure_ascii=False, indent=2)

            # ä¿å­˜ CSV
            if csv_logs:
                pd.DataFrame(csv_logs).to_csv(
                    os.path.join(out_folder, "progress_log.csv"),
                    index=False,
                    encoding="utf-8-sig",
                )

    def _append_to_folder_logs(self, output_dir, frontend_data, csv_data):
        """
        è¾…åŠ©å‡½æ•°ï¼šå‘æŒ‡å®š folders çš„ logs è¿½åŠ æ•°æ®ã€‚
        å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œå­˜åœ¨åˆ™è¯»å–åè¿½åŠ  (é¿å…è¦†ç›–åŒç›®å½•ä¸‹çš„å…¶ä»–æ–‡ä»¶è®°å½•)
        """

        # === 1. å¤„ç† JSON (frontend_diff.json) ===
        json_path = os.path.join(output_dir, "frontend_diff.json")
        current_list = []

        # è¯»å–æ—§æ•°æ®
        if os.path.exists(json_path):
            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    if content:
                        current_list = json.loads(content)
            except Exception as e:
                print(f"âš ï¸ Error reading JSON log: {e}")

        # å»é‡æ›´æ–°ï¼šå¦‚æœåˆ—è¡¨ä¸­å·²ç»æœ‰äº†è¿™ä¸ª filenameï¼Œå…ˆåˆ æ‰æ—§çš„ï¼Œå†åŠ æ–°çš„
        current_list = [
            item
            for item in current_list
            if item.get("filename") != frontend_data["filename"]
        ]
        current_list.append(frontend_data)

        # å†™å…¥
        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(current_list, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ Failed to write JSON log: {e}")

        # === 2. å¤„ç† CSV (progress_log.csv) ===
        csv_path = os.path.join(output_dir, "progress_log.csv")
        df = pd.DataFrame()

        # è¯»å–æ—§æ•°æ®
        if os.path.exists(csv_path):
            try:
                df = pd.read_csv(csv_path)
            except Exception as e:
                print(f"âš ï¸ Error reading CSV log: {e}")

        new_row = pd.DataFrame([csv_data])

        # å»é‡æ›´æ–°
        if not df.empty and "Filename" in df.columns:
            # åˆ é™¤æ—§è®°å½•
            df = df[df["Filename"] != csv_data["Filename"]]
            # è¿½åŠ æ–°è®°å½•
            df = pd.concat([df, new_row], ignore_index=True)
        else:
            # å¦‚æœæ˜¯ç©ºè¡¨ï¼Œç›´æ¥èµ‹å€¼
            df = new_row

        # å†™å…¥ (utf-8-sig é˜²æ­¢ Excel æ‰“å¼€ä¹±ç )
        try:
            df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        except Exception as e:
            print(f"âŒ Failed to write CSV log: {e}")

    def dispose(self):
        print("ğŸ—‘ï¸ Disposing Pipeline resources...")
        if hasattr(self, "cleaner"):
            self.cleaner.release_memory()
        if hasattr(self, "semantic_filter"):
            self.semantic_filter.release_memory()
        self.cleaner = None
        self.semantic_filter = None
        self.seen_hashes = None
        gc.collect()
        print("âœ¨ Pipeline resources completely freed.")


if __name__ == "__main__":
    # è·¯å¾„é…ç½®
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = current_script_dir

    # åŠ¨æ€æ‹¼æ¥æ¨¡å‹è·¯å¾„
    noise_model_path = os.path.join(
        project_root, "models", "noise-cleaner-deberta-v2", "final"
    )
    semantic_model_path = os.path.join(project_root, "models", "all-MiniLM-L6-v2")

    MODEL_CONFIGS = {
        "NOISE_CAPTION": noise_model_path,
        "SEMANTIC_MODEL": semantic_model_path,
    }

    INPUT_DIR = os.path.join(project_root, "Corpus")
    OUTPUT_DIR = os.path.join(project_root, "Cleaned_Corpus")

    if not os.path.exists(INPUT_DIR):
        print(f"âš ï¸ Input Directory not found: {INPUT_DIR}")
    else:
        pipeline = CorpusPipeline(MODEL_CONFIGS)
        try:
            pipeline.process_folder(INPUT_DIR, OUTPUT_DIR)
        finally:
            pipeline.dispose()
